{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\nfrom sklearn import preprocessing\nfrom tqdm.notebook import tqdm\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport glob\nimport os\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder \nfrom tqdm.notebook import tqdm\nimport optuna\n\nimport gc, datetime, random\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom collections import defaultdict, Counter\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn.model_selection import StratifiedKFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# helper funcs"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef reduce_mem(df, verbose=False):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef reduce_mem_usage(df, verbose=False):\n    return reduce_mem(df, verbose=verbose)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess1(df, train_flag):\n#     cat_cols = cat_cols\n    cat_cols = ['City_Code', 'Region_Code', 'Accomodation_Type',\n       'Reco_Insurance_Type', 'Upper_Age', 'Lower_Age', 'Is_Spouse',\n       'Health Indicator', 'Holding_Policy_Duration', 'Holding_Policy_Type',\n       'Reco_Policy_Cat']\n    cate_cols = cat_cols\n    feature_key = cat_cols\n    feature_target = cat_cols\n    for f in tqdm(cate_cols):\n        map_dict = dict(zip(df[f].unique(), range(df[f].nunique())))\n        df[f + '_count'] = df[f].map(df[f].value_counts())\n    df = reduce_mem(df)\n    ##########################groupby feature#######################\n    def group_fea(df,key,target):\n        tmp = df.groupby(key, as_index=False)[target].agg({\n            key+'_'+target + '_nunique': 'nunique',\n        }).reset_index()\n        del tmp['index']\n        return tmp\n\n    for key in tqdm(feature_key):\n        for target in feature_target:\n            tmp = group_fea(df,key,target)\n            df = df.merge(tmp,on=key,how='left')\n            \n            \n    #统计做了groupby特征的特征\n    group_list = []\n    for s in df.columns:\n        if '_nunique' in s:\n            group_list.append(s)\n    #print(group_list)\n    return df, group_list\n\ndef preprocess(train_df, test_df, num_clusters=10):\n\n    cat_cols = ['City_Code', 'Region_Code', 'Accomodation_Type',\n       'Reco_Insurance_Type', 'Upper_Age', 'Lower_Age', 'Is_Spouse',\n       'Health Indicator', 'Holding_Policy_Duration', 'Holding_Policy_Type',\n       'Reco_Policy_Cat']\n    enc_list=[]\n    \n    \n    \n#     test_df, _ = preprocess1(test_df, 0)\n#     train_df, group_list = preprocess1(train_df.copy(), 1)\n    \n#     skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=2020)\n#     enc_list = group_list + cat_cols\n    \n#     for f in tqdm(enc_list):\n#         if len(train_df[f].unique()) > len(train_df)//100:\n#             print(\"f \", f)\n#             continue\n#         train_df[f + '_target_enc'] = 0\n#         test_df[f + '_target_enc'] = 0\n#         for i, (trn_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):\n#             trn_x = train_df[[f, 'label']].iloc[trn_idx].reset_index(drop=True)\n#             val_x = train_df[[f]].iloc[val_idx].reset_index(drop=True)\n#             enc_df = trn_x.groupby(f, as_index=False)['label'].agg({f + '_target_enc': 'mean'})\n#             val_x = val_x.merge(enc_df, on=f, how='left')\n#             test_x = test_df[[f]].merge(enc_df, on=f, how='left')\n#             val_x[f + '_target_enc'] = val_x[f + '_target_enc'].fillna(train_df['label'].mean())\n#             test_x[f + '_target_enc'] = test_x[f + '_target_enc'].fillna(train_df['label'].mean())\n#             train_df.loc[val_idx, f + '_target_enc'] = val_x[f + '_target_enc'].values\n#             test_df[f + '_target_enc'] += test_x[f + '_target_enc'].values / skf.n_splits\n#             gc.collect()\n#         gc.collect()\n        \n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True), enc_list\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/job-a-thon-rw/train_Df64byy.csv\")\ntest = pd.read_csv(\"/kaggle/input/job-a-thon-rw/test_YCcRUnU.csv\")\nsub = pd.read_csv(\"/kaggle/input/job-a-thon-rw/sample_submission_QrCyCoT.csv\")\nsub['ID'] = test['ID']\n\n\ntrain = train.fillna(-1)\ntest = test.fillna(-1)\n\n\ntrain = train.drop(['ID'], axis=1)\ntest = test.drop(['ID'], axis=1)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn = [\n# 'ID',\n 'City_Code',\n 'Region_Code',\n 'Accomodation_Type',\n 'Reco_Insurance_Type',\n 'Upper_Age',\n 'Lower_Age',\n 'Is_Spouse',\n 'Health Indicator',\n 'Holding_Policy_Duration',\n 'Holding_Policy_Type',\n 'Reco_Policy_Cat',\n 'Reco_Policy_Premium',\n 'label']\n\ntarget = 'label'\ntrain.columns = nn\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = ['City_Code',\n 'Region_Code',\n 'Accomodation_Type',\n 'Reco_Insurance_Type',\n 'Is_Spouse',\n 'Health Indicator',\n 'Holding_Policy_Duration',\n 'Holding_Policy_Type',\n 'Reco_Policy_Cat']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nenc.fit(np.array(train['Reco_Policy_Cat']).reshape(-1, 1))\ndf_tr = pd.DataFrame(enc.transform(np.array(train['Reco_Policy_Cat']).reshape(-1, 1)))\ndf_tes = pd.DataFrame(enc.transform(np.array(test['Reco_Policy_Cat']).reshape(-1, 1)))\ntr_col = []\nfor i in df_tr.columns:\n    tr_col.append(\"OH_\" + str(i))\ndf_tr.columns = tr_col\n\ntr_col = []\nfor i in df_tes.columns:\n    tr_col.append(\"OH_\" + str(i))\ndf_tes.columns = tr_col\n\n\ntrain = pd.concat([train, df_tr], axis=1)\ntest = pd.concat([test, df_tes], axis=1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i='Region_Code'\nxx = train[target].value_counts()[0] / train[target].value_counts()[1] \ndf = pd.DataFrame({0 : train[train[target]==0][i].value_counts()/xx, 1: train[train[target]==1][i].value_counts()})\nplt.plot(np.abs((df[0]-df[1] )/df.min(axis=1))*100)\n\nidx = list((np.abs((df[0]-df[1] )/df.min(axis=1))*100).sort_values(ascending=False)[:200].index)\n\ntrain['Region_Code_limited'] = train['Region_Code']\ntrain['Region_Code_limited']\nfor j in train['Region_Code_limited'].index:\n    if train['Region_Code_limited'][j] in idx:\n        continue\n    else :\n        train['Region_Code_limited'][j] = -1\n        \n        \ntest['Region_Code_limited'] = test['Region_Code']\ntest['Region_Code_limited']\nfor j in test['Region_Code_limited'].index:\n    if test['Region_Code_limited'][j] in idx:\n        continue\n    else :\n        test['Region_Code_limited'][j] = -1\n        \n        \ncat_cols.append('Region_Code_limited')\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['Region_Code_limited'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# balaced check"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['Response'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train1 = train[train['Response'] == 1]\n# train = pd.concat([train, train1], axis=0).reset_index(drop=True)\n# train = pd.concat([train, train1], axis=0).reset_index(drop=True)\n# ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['Response'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in train.columns:\n#     print(i,  len(train[i].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_validation = train_test_split(train, train_size=0.80000, random_state=42)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_validation, enc_list = preprocess(X_train.copy(), X_validation.copy(), num_clusters=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_validation = X_validation[target]\ny_train = X_train[target]\n\nX_train = X_train.drop(target, axis=1)\nX_validation = X_validation.drop(target, axis=1)\nX_validation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_new, test, enc_list = preprocess(train.copy(), test.copy(), num_clusters=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tuning"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CAtbOost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# FYI: Objective functions can take additional arguments\n# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\ndef objective(trial):\n    param = {\n        \"eval_metric\": \"AUC\",\n        'logging_level': 'Silent',\n        'boosting_type' : 'Plain' ,\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-2, 0.5, log=True),\n        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-8, 500.0, log=True),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 12, log=False),\n        'task_type':\"GPU\",       \n        \"iterations\": trial.suggest_int(\"iterations\", 50, 5000),\n        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 1e-2, 0.5, log=True),\n        \"max_ctr_complexity\": trial.suggest_int(\"max_ctr_complexity\", 2, 2),\n        \"max_bin\": trial.suggest_int(\"max_bin\", 32, 200),\n        \"use_best_model\" : False,\n    }\n    \n    \n    model = CatBoostClassifier(**param)\n    model = model.fit(\n                        X_train, y_train, \n                        cat_features=cat_cols,\n                        use_best_model=False,\n#                         eval_set=(X_validation, y_validation),\n                     )\n    preds = model.predict_proba(X_validation)[:, 1]\n    accuracy = sklearn.metrics.roc_auc_score(y_validation, preds)\n    return accuracy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'learning_rate': 0.0769396280700289,\n 'l2_leaf_reg': 117.9397659497423,\n 'max_depth': 5,\n 'iterations': 3465,\n 'bagging_temperature': 0.023880907815556483,\n 'max_ctr_complexity': 2,\n 'max_bin': 154}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # study = optuna.create_study(direction=\"minimize\")\n# study = optuna.create_study(direction=\"maximize\")\n# # increase n_trails to test more params\n# study.optimize(objective, n_trials=60)\n\n# print(\"Number of finished trials: {}\".format(len(study.trials)))\n# trial = study.best_trial\n# print(\"Best trial:\")\n# # paramss.append(trial.params)\n\n# print(\"  Value: {}\".format(trial.value))\n\n# print(\"  Params: \")\n\n\n# for key, value in trial.params.items():\n#     print(\"    {}: {}\".format(key, value))\n    \n\n# params = trial.params\n# trial.params\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nparams['eval_metric'] = 'AUC'\nparams['logging_level'] = 'Silent'\nparams['boosting_type'] = 'Plain'\n# params['task_type'] = \"GPU\"\n\n\nmodel = CatBoostClassifier(**params)\nmodel.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_list = [\n#     {'learning_rate': 0.03504690553120952, 'l2_leaf_reg': 16.674632013331745, 'max_depth': 9, 'iterations': 67, 'bagging_temperature': 0.018863676053720923, 'max_ctr_complexity': 2, 'max_bin': 190},\n#     {'learning_rate': 0.4976951327091617, 'l2_leaf_reg': 0.06382353714637816, 'max_depth': 4, 'iterations': 3288, 'bagging_temperature': 0.012422864451370747, 'max_ctr_complexity': 2, 'max_bin': 38},\n#     {'learning_rate': 0.03417560009829608, 'l2_leaf_reg': 0.04325250648764644, 'max_depth': 9, 'iterations': 4064, 'bagging_temperature': 0.012711310396452059, 'max_ctr_complexity': 2, 'max_bin': 139},\n#     {'learning_rate': 0.017217004608133267, 'l2_leaf_reg': 2.490993431649518e-06, 'max_depth': 12, 'iterations': 1513, 'bagging_temperature': 0.13830457550750483, 'max_ctr_complexity': 2, 'max_bin': 71},\n    \n    # last col removed \n    {'learning_rate': 0.1197155786554287, 'l2_leaf_reg': 1.792618447708568, 'max_depth': 3, 'iterations': 3979, 'bagging_temperature': 0.03598742456824714, 'max_ctr_complexity': 2, 'max_bin': 125},\n    {'learning_rate': 0.03544766725209928, 'l2_leaf_reg': 1.370803084483364, 'max_depth': 3, 'iterations': 3708, 'bagging_temperature': 0.03901618689549485, 'max_ctr_complexity': 2, 'max_bin': 124},\n    {'learning_rate': 0.03436720800438295, 'l2_leaf_reg': 0.09215163878772453, 'max_depth': 3, 'iterations': 3870, 'bagging_temperature': 0.020452925772898055, 'max_ctr_complexity': 2, 'max_bin': 88},\n    {'learning_rate': 0.1266276143771968, 'l2_leaf_reg': 2.0110845275197993, 'max_depth': 2, 'iterations': 782, 'bagging_temperature': 0.029945781198908217, 'max_ctr_complexity': 2, 'max_bin': 157},\n    {'learning_rate': 0.04104228366631473, 'l2_leaf_reg': 5.826887261802294, 'max_depth': 6, 'iterations': 2649, 'bagging_temperature': 0.06985674308863894, 'max_ctr_complexity': 2, 'max_bin': 173},\n    \n    \n    \n    {'learning_rate': 0.0769396280700289, 'l2_leaf_reg': 117.9397659497423, 'max_depth': 5, 'iterations': 3465, 'bagging_temperature': 0.023880907815556483, 'max_ctr_complexity': 2, 'max_bin': 154},\n    {'learning_rate': 0.12059119126628469, 'l2_leaf_reg': 0.1974485737651515, 'max_depth': 3, 'iterations': 3574, 'bagging_temperature': 0.012096758886745147, 'max_ctr_complexity': 2, 'max_bin': 66},\n    {'learning_rate': 0.06623001793892062, 'l2_leaf_reg': 28.646322073736556, 'max_depth': 3, 'iterations': 3542, 'bagging_temperature': 0.012274728415153181, 'max_ctr_complexity': 2, 'max_bin': 59},\n    {'learning_rate': 0.10277504026045622, 'l2_leaf_reg': 6.093172008641855, 'max_depth': 2, 'iterations': 3339, 'bagging_temperature': 0.36483302461032735, 'max_ctr_complexity': 2, 'max_bin': 44},\n    {'learning_rate': 0.016976100171073007, 'l2_leaf_reg': 1.8060489262789463, 'max_depth': 4, 'iterations': 4134, 'bagging_temperature': 0.4785455551994752, 'max_ctr_complexity': 2, 'max_bin': 94},\n    {'learning_rate': 0.039148597638397585, 'l2_leaf_reg': 22.14201754863478, 'max_depth': 3, 'iterations': 3915, 'bagging_temperature': 0.07737592690958507, 'max_ctr_complexity': 2, 'max_bin': 77},\n    \n    {'learning_rate': 0.028883448110346648, 'l2_leaf_reg': 482.9056272005946, 'max_depth': 5, 'iterations': 4291, 'bagging_temperature': 0.02350408621458312, 'max_ctr_complexity': 2, 'max_bin': 54},\n    {'learning_rate': 0.02260331189053014, 'l2_leaf_reg': 101.03612749341279, 'max_depth': 2, 'iterations': 3026, 'bagging_temperature': 0.03082718141487049, 'max_ctr_complexity': 2, 'max_bin': 60},\n    {'learning_rate': 0.028391019999274084, 'l2_leaf_reg': 1.6714091660819037, 'max_depth': 4, 'iterations': 3588, 'bagging_temperature': 0.021733070715488616, 'max_ctr_complexity': 2, 'max_bin': 54},\n    {'learning_rate': 0.055396516759364335, 'l2_leaf_reg': 0.7207341110153501, 'max_depth': 4, 'iterations': 3877, 'bagging_temperature': 0.01649717469060937, 'max_ctr_complexity': 2, 'max_bin': 34},\n    {'learning_rate': 0.10782034981327523, 'l2_leaf_reg': 36.11128296358572, 'max_depth': 6, 'iterations': 2273, 'bagging_temperature': 0.016578649076498257, 'max_ctr_complexity': 2, 'max_bin': 112},\n]\n\n#last 10 diff params : non bal less col versio 9","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nno_of_stacks = 1\nstack_df = pd.DataFrame(index = train.index)\nstack_df[target] = train[target]\n\n\nstack_df_test = pd.DataFrame(index = test.index)\n# stack_df_test[target] = train[target]\n\n\n\nfor no_of_stack in tqdm(range(no_of_stacks)):\n    stack_df[no_of_stack] = 0.0\n    params = params_list[no_of_stack]\n    \n    params = {'learning_rate': 0.03135940484917028, 'l2_leaf_reg': 301.1739543936941, 'max_depth': 8, 'iterations': 3682, 'bagging_temperature': 0.14945648119943328, 'max_ctr_complexity': 2, 'max_bin': 54}\n\n    params['eval_metric'] = 'AUC'\n    params['logging_level'] = 'Silent'\n    params['boosting_type'] = 'Plain'\n#     params['task_type'] = \"GPU\"\n\n    model = CatBoostClassifier(**params)\n    model.get_params()\n    \n    \n    \n    n_folds = 20\n    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n    y_pred = np.array([0.0] * len(test))\n    \n    for i, (trn_idx, val_idx) in enumerate(skf.split(train, train[target])):\n        X_train = train.loc[trn_idx, :].drop(target, axis=1)#.reset_index(drop=True)\n        X_validation = train.loc[val_idx, :].drop(target, axis=1)\n        y_train = train.loc[trn_idx, target]\n        y_validation = train.loc[val_idx, target]\n\n        X_train, X_validation, enc_list = preprocess(X_train.copy(), X_validation.copy(), num_clusters=10)\n        #test is already preprocessed.\n        \n        X_train_t, X_valid_t, y_train_t, y_valid_t = train_test_split(X_train, y_train, train_size=0.80000, random_state=42)\n\n        \n        #choose eval_Set or not to go with eval set\n        model.fit(\n            X_train, y_train,\n            cat_features=cat_cols,\n#             eval_set=(X_valid_t, y_valid_t),\n            plot=True\n        )\n        \n        pred_X_validation = model.predict_proba(X_validation)[:, 1]\n        stack_df.loc[val_idx, no_of_stack] = pred_X_validation\n        \n        print(no_of_stack, i, sklearn.metrics.roc_auc_score(y_validation, model.predict_proba(X_validation)[:, 1]))\n\n        y_pred += model.predict_proba(test)[:, 1]/(n_folds)\n        gc.collect()\n        \n    stack_df_test[no_of_stack] = y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n_folds = 10\n# skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# y_pred = np.array([0.0] * len(test))\n\n# for i, (trn_idx, val_idx) in enumerate(skf.split(train, train[target])):\n#     X_train = train.loc[trn_idx, :].drop(target, axis=1)#.reset_index(drop=True)\n#     X_validation = train.loc[val_idx, :].drop(target, axis=1)\n#     y_train = train.loc[trn_idx, target]\n#     y_validation = train.loc[val_idx, target]\n    \n    \n#     X_train, X_validation, enc_list = preprocess(X_train.copy(), X_validation.copy(), num_clusters=10)\n#     #test is already preprocessed.\n    \n        \n#     model.fit(\n#     X_train, y_train,\n#     cat_features=cat_cols,\n#     eval_set=(X_validation, y_validation),\n#     plot=True\n#     )\n    \n#     y_pred += model.predict_proba(test)[:, 1]/n_folds\n\n    \n#     gc.collect()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n_folds = 5\n# skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# y_pred = np.array([0.0] * len(test))\n\n# for i, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n#     X_train_t = X_train.loc[trn_idx, :]#.drop(target, axis=1)#.reset_index(drop=True)\n#     X_valid_t = X_train.loc[val_idx, :]#.drop(target, axis=1)\n#     y_train_t = y_train.loc[trn_idx]\n#     y_valid_t = y_train.loc[val_idx]\n    \n    \n#     model.fit(\n#         X_train_t, y_train_t,\n#         cat_features=cat_cols,\n#         eval_set=(X_valid_t, y_valid_t),\n#         plot=True\n#     )\n    \n#     print(i, sklearn.metrics.roc_auc_score(y_validation, model.predict_proba(X_validation)[:, 1]))\n    \n#     y_pred += model.predict_proba(test)[:, 1]/n_folds\n\n    \n#     gc.collect()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # X_train, X_validation, y_train, y_validation, test, enc_list = preprocess(df, train_flag=1)\n# model.fit(\n#     X_train, y_train,\n#     cat_features=cat_cols,\n#     eval_set=(X_validation, y_validation),\n#     plot=True\n# )\n\n# y_pred = model.predict_proba(test)[:, 1]\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # y_pred = model.predict_proba(test_df)[:, 1]\n# sub['Response'] = y_pred\n# sub.to_csv('sub_job_a_thon.csv', index=False)\n# sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLinks\nFileLinks('.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# LGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"daa = pd.concat([X_train, X_validation, test], axis=0)\n# daa\nfor i  in  ['City_Code', 'Accomodation_Type', 'Reco_Insurance_Type', 'Is_Spouse', 'Health Indicator', 'Holding_Policy_Duration']:\n    le = LabelEncoder()\n#     print(daa[i][0].dtype)\n\n    daa[i] = le.fit(daa[i].astype(str))\n    X_train[i] = le.transform(X_train[i].astype(str))\n    X_validation[i] = le.transform(X_validation[i].astype(str))\n    test[i] = le.transform(test[i].astype(str))\n\ncat_col_lgb = []\nfor e, i in enumerate(X_train.columns):\n    if i in cat_cols:\n        cat_col_lgb.append(e)\ncat_col_lgb\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FYI: Objective functions can take additional arguments\n# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\ndef objective(trial):\n    #data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n#     file = \"../input/indoornavigationandlocationsum-xyz/train/5a0546857ecc773753327266_1000_train.csv\"\n    file = train_files[i]\n    data = pd.read_csv(file, )\n    print(i)\n    \n    x_train = data.drop(['x', 'y', 'f', 'path'], axis=1)\n    le = LabelEncoder()\n    data['f'] = le.fit_transform(data['f'])\n    y_trainy = data.loc[:,'f']\n    \n    train_x, valid_x, train_y, valid_y = train_test_split(x_train, y_trainy, test_size=0.25)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n#     print(valid_y.value_counts())\n#     print(train_y.value_counts())\n    \n    param = {\n        \"objective\": \"multiclass\",\n        \"metric\": \"multi_logloss\",\n        \"num_class\" : len(data.loc[:,'f'].unique()),\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n    \n\n    \n    \n    \n    \n    \n    \n    \n    gbm = lgb.train(param, dtrain)\n    preds = gbm.predict_proba(valid_x)[:, 1]\n    pred_labels = (preds)\n    accuracy = np.sqrt(sklearn.metrics.roc_auc_score(valid_y, pred_labels))\n#     accuracy = np.sqrt(sklearn.metrics.accuracy_score(valid_y, pred_labels))\n    \n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FYI: Objective functions can take additional arguments\n# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\ndef objective(trial):\n    \n    params = {\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 5000),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n        'cat_feature':cat_col_lgb, \n        'verbose':-1,\n        \n    }\n    \n#     model = CatBoostClassifier(**param)\n#     model = model.fit(\n#                         X_train, y_train, \n#                         cat_features=cat_cols,\n#                         use_best_model=False,\n# #                         eval_set=(X_validation, y_validation),\n#                      )\n    \n    \n    model = lgb.LGBMClassifier(**params)\n    model.fit(\n        X_train, y_train,\n        verbose=-1,\n        \n    )\n    preds = model.predict_proba(X_validation)[:, 1]\n    accuracy = sklearn.metrics.roc_auc_score(y_validation, preds)\n    return accuracy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'learning_rate': 0.02039671552078551, 'l2_leaf_reg': 0.9538065032003882, 'max_depth': 8, 'iterations': 1500, 'bagging_temperature': 0.29925486960824327, 'max_bin': 38}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# study = optuna.create_study(direction=\"minimize\")\nstudy = optuna.create_study(direction=\"maximize\")\n# increase n_trails to test more params\nstudy.optimize(objective, n_trials=30)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\ntrial = study.best_trial\nprint(\"Best trial:\")\n# paramss.append(trial.params)\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\n\n\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n    \nparams = trial.params\ntrial.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params['metric'] = 'auc'\nparams['verbose'] = -1\nparams['boosting_type'] = 'gbdt'\nparams['cat_feature'] = cat_col_lgb\n\n\nmodel = lgb.LGBMClassifier(**params)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# n_folds = 5\n# skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# y_pred = np.array([0.0] * len(test))\n\n# for i, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n#     X_train_t = X_train.loc[trn_idx, :]#.drop(target, axis=1)#.reset_index(drop=True)\n#     X_valid_t = X_train.loc[val_idx, :]#.drop(target, axis=1)\n#     y_train_t = y_train.loc[trn_idx]\n#     y_valid_t = y_train.loc[val_idx]\n    \n#     model.fit(\n#         X_train_t, y_train_t,\n#         eval_set=(X_valid_t, y_valid_t),\n#         verbose=-1,       \n#     )\n    \n#     print(i, sklearn.metrics.roc_auc_score(y_validation, model.predict_proba(X_validation)[:, 1]))\n#     y_pred += model.predict_proba(test)[:, 1]/n_folds\n\n    \n#     gc.collect()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.fit(\n#     X_train, y_train,\n#     eval_set=(X_validation, y_validation),\n#     verbose=-1,\n# )\n# y_pred = model.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# tpot"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tpot import TPOTClassifier\ntp = TPOTClassifier()\ntpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42, scoring='roc_auc')\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_validation, y_validation))\ntpot.export('tpot_iris_pipeline.py')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tpot.export('tpot_iris_pipeline.py')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_pred = model.predict_proba(test_df)[:, 1]\nsub['Response'] = y_pred\nsub.to_csv('sub_job_a_thon.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLinks\nFileLinks('.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}